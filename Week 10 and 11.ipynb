{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10/11: Spam Text Messgae Filter \n",
    "\n",
    "For this week's assignment I will be looking at a text messgaes that have been labeled as either spam or ham (not spam). The idea is to split the total group into training and test sets and then run Naive Bayes Classifier on the test set to see if it can guess which messages are spam or ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer as Stemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "I've uploaded the text messgaes file to my githup here: https://github.com/dquarshie89/Data-620/blob/master/spam.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     v1                                                 v2\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "747\n",
      "4825\n"
     ]
    }
   ],
   "source": [
    "spam = pd.read_csv('spam.csv', header = 0, encoding='latin-1')\n",
    "\n",
    "#Remove unwanted columns\n",
    "spam=spam.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'])\n",
    "\n",
    "#Preview data\n",
    "print(spam.head(5))\n",
    "\n",
    "#See how many spam messages there are\n",
    "print(len(spam[spam.v1=='spam']))\n",
    "\n",
    "#See how many ham messages there are\n",
    "print(len(spam[spam.v1=='ham']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the csv read we will clean up the messages so that our predictor can work with them. We'll make all the text lower case, get rid of any punctuations, remove stopwords, and allow stem sentences. Stemming is the process of shorting long unneeded words or sentences. For example fishy or fish-like will simply become fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    text = text.lower() #Make all the text lowercase\n",
    "    text = ''.join([t for t in text if t not in string.punctuation]) #Get rid of puntucations\n",
    "    text = [t for t in text.split() if t not in stopwords.words('english')] #Remove stop words\n",
    "    st = Stemmer() #Stem sentences to reduce inflections(https://en.wikipedia.org/wiki/Stemming)\n",
    "    text = [st.stem(t) for t in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer\n",
    "TfidfVectorizer (term frequencyâ€“inverse document frequency) is a method that will convert our text message file into a 2D matrix and see which words are most important. It will seee how often a word shows up and then proceed to give the word a weighted score. We'll use these scores in our prediction to see which text messages are spam or ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer(cleaning)\n",
    "data = tfidfv.fit_transform(spam['v2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Withv our vectors set up and scores in place we're ready to make our classifier. We can use MultinomialNB to train our set using the TFIDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_filter = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(cleaning)), #weighted TFIDF score\n",
    "    ('classifier', MultinomialNB()) #train on TFIDF vectors with Naive Bayes\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "        input=<function cleaning at 0x1a0d94bd90>, lowercase=True,\n",
       "        max_df=1.0, max_features=None, min_df=1, ngram_range=(1, 1),\n",
       "        norm='l2'...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(spam['v2'], spam['v1'], test_size=0.2)\n",
    "\n",
    "text_filter.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test set\n",
    "predictions = text_filter.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that NB has ran predictions to see which messages were spam or ham let's see how many in actually got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test cases: 1115\n",
      "Number of correct of predictions: 1071\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test.iloc[i] == predictions[i]:\n",
    "        count += 1\n",
    "        \n",
    "print('Number of test cases:', len(y_test))\n",
    "print('Number of correct of predictions:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our NB predictor was able to get 1071 messages labeled correctly, 96%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Our method was successful in getting 96% of the predictions correct. Having a small dataset set us a back a bit and using Naive Bayes which assumes features are indepent of each other may have been a drawback. However getting 96% correct is great and shows that NB can be used to help us label what we'd want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
